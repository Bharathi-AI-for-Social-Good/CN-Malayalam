# -*- coding: utf-8 -*-
"""ReZG_Train+Gen_Full_Code.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1cgtfToOYgW1w-nlSgB6aQ_IYhdDKhdiR
"""

import pandas as pd
from sklearn.model_selection import train_test_split
from datasets import Dataset
from transformers import XLMRobertaTokenizer, XLMRobertaForSequenceClassification, Trainer, TrainingArguments
from transformers import DataCollatorWithPadding
import numpy as np
import torch
from sklearn.metrics import accuracy_score, precision_recall_fscore_support
import os

# Disable Weights & Biases logging
os.environ["WANDB_DISABLED"] = "true"

# Load the dataset
file_path = "5100GPTDatasetHITL.csv.csv"
df = pd.read_csv(file_path)

# Prepare data
data = pd.DataFrame({
    "text": df["H/T"].astype(str).tolist() + df["CS"].astype(str).tolist(),
    "label": [0] * len(df) + [1] * len(df)  # 0 = HATE, 1 = COUNTER
}).sample(frac=1.0, random_state=42).reset_index(drop=True)

# Train-test split
train_df, test_df = train_test_split(data, test_size=0.2, stratify=data["label"])

# Tokenizer
tokenizer = XLMRobertaTokenizer.from_pretrained("xlm-roberta-base")

def tokenize_function(examples):
    return tokenizer(examples["text"], truncation=True)

# Convert to Hugging Face Datasets
train_ds = Dataset.from_pandas(train_df)
test_ds = Dataset.from_pandas(test_df)
train_ds = train_ds.map(tokenize_function, batched=True)
test_ds = test_ds.map(tokenize_function, batched=True)

# Model
model = XLMRobertaForSequenceClassification.from_pretrained("xlm-roberta-base", num_labels=2)

# Data collator
data_collator = DataCollatorWithPadding(tokenizer=tokenizer)

# Metrics
def compute_metrics(eval_pred):
    logits, labels = eval_pred
    predictions = np.argmax(logits, axis=-1)
    precision, recall, f1, _ = precision_recall_fscore_support(labels, predictions, average="binary")
    acc = accuracy_score(labels, predictions)
    return {"accuracy": acc, "f1": f1, "precision": precision, "recall": recall}

# Training arguments
training_args = TrainingArguments(
    output_dir="./results",
    eval_strategy="epoch",
    save_strategy="epoch",
    learning_rate=2e-5,
    per_device_train_batch_size=8,
    per_device_eval_batch_size=8,
    num_train_epochs=20,
    weight_decay=0.01,
    save_total_limit=1,
    load_best_model_at_end=True,
)

# Trainer
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_ds,
    eval_dataset=test_ds,
    tokenizer=tokenizer,
    data_collator=data_collator,
    compute_metrics=compute_metrics,
)

# Train and save
trainer.train()
model.save_pretrained("saved_model")
tokenizer.save_pretrained("saved_model")



import shutil

# Zip the folder
shutil.make_archive('saved_model', 'zip', 'saved_model')

# Download the zipped folder
from google.colab import files
files.download('saved_model.zip')

!pip install faiss-cpu

# Convert your dataset to JSONL format suitable for fine-tuning

output_path = "fine_tune_data.jsonl"

# Filter and rename
fine_tune_data = df[["H/T", "CS"]].dropna()
fine_tune_data = fine_tune_data.rename(columns={"H/T": "instruction", "CS": "response"})

# Format instructions
fine_tune_data["instruction"] = fine_tune_data["instruction"].apply(
    lambda x: f"ഹേറ്റുകൾ: {x.strip()}"
)

# Export as JSONL
fine_tune_data.to_json(output_path, orient="records", lines=True, force_ascii=False)

# Display path for download
output_path

!pip install transformers datasets peft accelerate bitsandbytes

!pip install -U bitsandbytes
!pip install -U transformers accelerate peft

from datasets import Dataset
from transformers import (
    AutoTokenizer, AutoModelForCausalLM,
    TrainingArguments, Trainer
)
from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training
import pandas as pd
import os
import torch

# Disable WandB
os.environ["WANDB_DISABLED"] = "true"

# ✅ Load the dataset
df = pd.read_json("fine_tune_data.jsonl", lines=True)
dataset = Dataset.from_pandas(df)

# Optional: Remove index column
if "__index_level_0__" in dataset.column_names:
    dataset = dataset.remove_columns("__index_level_0__")

# ✅ Format as instruction prompt
def format_prompt(example):
    return f"വാക്യത്തിൽ തെളിയിക്കുന്ന വെറുപ്പിന് എതിരായ ഒരു മറുപടി എഴുതുക.\n\n{example['instruction']}\n\nമറുപടി: {example['response']}"

dataset = dataset.map(lambda x: {"text": format_prompt(x)})

# ✅ Load tokenizer and model
model_name = "VinkuraAI/KunoRZN-Llama-3-3B"
tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=False)
model = AutoModelForCausalLM.from_pretrained(model_name, load_in_4bit=True, device_map="auto")

# ✅ Tokenization with labels
def tokenize(example):
    result = tokenizer(
        example["text"],
        truncation=True,
        padding="max_length",
        max_length=512,
    )
    result["labels"] = result["input_ids"].copy()
    return result

tokenized_dataset = dataset.map(tokenize, batched=True)

# ✅ Apply QLoRA configuration
model = prepare_model_for_kbit_training(model)
lora_config = LoraConfig(
    r=8,
    lora_alpha=16,
    target_modules=["q_proj", "v_proj"],
    lora_dropout=0.05,
    bias="none",
    task_type="CAUSAL_LM"
)
model = get_peft_model(model, lora_config)

# ✅ Training arguments
training_args = TrainingArguments(
    output_dir="kuno-finetuned",
    per_device_train_batch_size=2,
    gradient_accumulation_steps=4,
    num_train_epochs=3,
    learning_rate=2e-4,
    fp16=True,
    save_strategy="epoch",
    logging_dir="./logs",
    save_total_limit=2,
)

# ✅ Trainer setup
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_dataset,
    tokenizer=tokenizer
)

# ✅ Train!
trainer.train()

# ✅ Save fine-tuned model
model.save_pretrained("kuno-finetuned")
tokenizer.save_pretrained("kuno-finetuned")

# ---- SSF-Based Counter-Speech Chatbot with KunoRZN-Llama-3-3B Generation ----

# Required installations (uncomment in Colab)
# !pip install transformers sentence-transformers faiss-cpu

import torch
import faiss
import numpy as np
import pandas as pd
import math
from sklearn.metrics.pairwise import cosine_similarity
from transformers import (
    XLMRobertaTokenizer, XLMRobertaForSequenceClassification,
    AutoTokenizer, AutoModelForCausalLM
)
from sentence_transformers import SentenceTransformer



# Load dataset
df = pd.read_csv("5100GPTDatasetHITL.csv.csv")
cs_list = df["CS"].astype(str).tolist()

# === 1. Semantic Similarity ===
embedder = SentenceTransformer("sentence-transformers/LaBSE")
cs_embeddings = embedder.encode(cs_list, convert_to_numpy=True, show_progress_bar=True)
index = faiss.IndexFlatL2(cs_embeddings.shape[1])
index.add(cs_embeddings)

def retrieve_topk_semantic(hs_text, top_k=30):
    hs_embedding = embedder.encode([hs_text], convert_to_numpy=True)
    distances, indices = index.search(hs_embedding, top_k)
    return [(cs_list[i], i) for i in indices[0]]

# === 2. Stance-Based Ranking ===
# Load stance classification model
stance_model_path = "./saved_model"
stance_tokenizer = XLMRobertaTokenizer.from_pretrained(stance_model_path)
stance_model = XLMRobertaForSequenceClassification.from_pretrained(stance_model_path)

# stance_tokenizer = XLMRobertaTokenizer.from_pretrained("xlm-roberta-base")
# stance_model = XLMRobertaForSequenceClassification.from_pretrained("xlm-roberta-base")
stance_model.eval()
stance_model.to("cuda" if torch.cuda.is_available() else "cpu")

def get_cls_embedding(text):
    inputs = stance_tokenizer(text, return_tensors="pt", truncation=True, padding=True, max_length=128)
    inputs = {k: v.to(stance_model.device) for k, v in inputs.items()}
    with torch.no_grad():
        outputs = stance_model.base_model(**inputs)
    return outputs.last_hidden_state[:, 0, :].squeeze().cpu().numpy()

def rank_by_stance(hs_text, retrieved_cs, top_k=10):
    hs_vec = get_cls_embedding(hs_text)
    cs_vecs = [get_cls_embedding(cs) for cs, _ in retrieved_cs]
    sims = cosine_similarity([hs_vec], cs_vecs)[0]
    sorted_indices = np.argsort(sims)
    return [retrieved_cs[i][0] for i in sorted_indices[:top_k]]

# === 3. Fitness (Perplexity) ===
from transformers import AutoModelForSeq2SeqLM
gen_tokenizer = AutoTokenizer.from_pretrained("google/mt5-small")
gen_model = AutoModelForSeq2SeqLM.from_pretrained("google/mt5-small")
gen_model.eval()
gen_model.to("cuda" if torch.cuda.is_available() else "cpu")

def compute_perplexity(prompt):
    encodings = gen_tokenizer(prompt, return_tensors="pt", truncation=True, padding=True, max_length=128)
    input_ids = encodings.input_ids.to(gen_model.device)
    with torch.no_grad():
        outputs = gen_model(input_ids=input_ids, labels=input_ids)
        loss = outputs.loss
    return math.exp(loss.item()) if loss is not None else float("inf")

def rank_by_fitness(hs_text, cs_candidates, top_k=10):
    prompt_template = lambda cs: f"{hs_text} അതിനെ എതിർക്കുന്നു: {cs}"
    ranked = [(cs, compute_perplexity(prompt_template(cs))) for cs in cs_candidates]
    return sorted(ranked, key=lambda x: x[1])[:top_k]

# === 4. Final Generation using VinkuraAI/KunoRZN-Llama-3-3B ===
# kuno_tokenizer = AutoTokenizer.from_pretrained("VinkuraAI/KunoRZN-Llama-3-3B")
# kuno_model = AutoModelForCausalLM.from_pretrained("VinkuraAI/KunoRZN-Llama-3-3B")
kuno_model = AutoModelForCausalLM.from_pretrained("kuno-finetuned")
kuno_tokenizer = AutoTokenizer.from_pretrained("kuno-finetuned")

kuno_model.eval()
kuno_model.to("cuda" if torch.cuda.is_available() else "cpu")

def generate_final_response(hs_text, top_cs):
    prompt = f"""ദയവായി ഒരു ശാന്തമായ, ബഹുമാനപരമായ, വിശദീകരണപരമായ മറുപടി തയ്യാറാക്കുക.
താഴെയുള്ള വെറുപ്പ് പ്രകടിപ്പിക്കുന്ന വാക്യത്തിന്ഒ, ഒരു അറിവ് അടിസ്ഥാനമാക്കിയുള്ള എതിര്‍പ് രേഖപ്പെടുത്തുക.
മറുപടി തർക്കം ക്ഷണിക്കാതെ, യുക്തിപൂർണ്ണവും മാന്യവുമായിരിക്കുക.

ഹേറ്റുകൾ: {hs_text}
ഉദാഹരണ പ്രതികരണം: {top_cs}

മറുപടി:"""
    inputs = kuno_tokenizer(prompt, return_tensors="pt").to(kuno_model.device)
    with torch.no_grad():
        output_ids = kuno_model.generate(
            **inputs,
            max_new_tokens=255,
            do_sample=True,
            top_k=50,
            top_p=0.95,
            temperature=0.7
        )
    return kuno_tokenizer.decode(output_ids[0], skip_special_tokens=True).split("മറുപടി:")[-1].strip()

# === Full Chatbot Function ===
def generate_counter_speech(hs_input):
    sem_candidates = retrieve_topk_semantic(hs_input, top_k=30)
    stance_filtered = rank_by_stance(hs_input, sem_candidates, top_k=10)
    top_fluent = rank_by_fitness(hs_input, stance_filtered, top_k=1)
    print(f"Top Fluent: {top_fluent}")
    if not top_fluent:
        return "ക്ഷമിക്കണം, അതിനായുള്ള മറുപടി കണ്ടെത്താൻ കഴിഞ്ഞില്ല."
    return generate_final_response(hs_input, top_fluent[0][0])

# # === Example Use ===
# user_input = "അവർക്ക് വിവാഹം കഴിക്കാനുള്ള അവകാശമില്ല"
# response = generate_counter_speech(user_input)
# print(f"🤖: {response}")

user_input = "അവർക്ക് വിവാഹം കഴിക്കാനുള്ള അവകാശമില്ല"
response = generate_counter_speech(user_input)
print(f"🤖: {response}")

print(f"🤖: {response[1]}")

!pip install detoxify

from detoxify import Detoxify
from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction
import nltk
nltk.download('punkt_tab')


# Load Detoxify
tox_model = Detoxify("original")
# generated_cs = "നിനിക്ക് അതുകൊണ്ടുതന്നെ, അവർ നിങ്ങൾക്ക് അത്രയിലേറ്റിൽ ആഗ്രഹിക്കുന്ന വിധത്തിൽ ജീവിക്കാനുള്ള അവകാശവും ഉള്ളതായിരിക്കുകയില്ല. ഒരു വിവാഹത്തിൽ പങ്കെടുക്കുന്നത് അവ"

def clean_generated_text(text):
    if "." in text:
        return text.rsplit(".", 1)[0] + "."
    return text  # fallback

safe_cs = clean_generated_text(response)

# ---- Evaluation Functions ----
def compute_relevance(hs_text, cs_text):
    hs_vec = embedder.encode([hs_text])
    cs_vec = embedder.encode([cs_text])
    return cosine_similarity(hs_vec, cs_vec)[0][0]

def compute_novelty(cs_text, training_cs_list):
    cs_vec = embedder.encode([cs_text])
    train_vecs = embedder.encode(training_cs_list)
    sims = cosine_similarity(cs_vec, train_vecs)[0]
    return 1 - np.max(sims)

def compute_bleu(reference, candidate):
    ref_tokens = [nltk.word_tokenize(reference)]
    cand_tokens = nltk.word_tokenize(candidate)
    return sentence_bleu(ref_tokens, cand_tokens, smoothing_function=SmoothingFunction().method1)

def compute_toxicity(text):
    return tox_model.predict(text).get("toxicity", 0.0)

def compute_fluency(text):
    inputs = gen_tokenizer(text, return_tensors="pt", truncation=True, padding=True, max_length=128).to(gen_model.device)
    with torch.no_grad():
        outputs = gen_model(input_ids=inputs["input_ids"], labels=inputs["input_ids"])
        loss = outputs.loss
    return np.exp(loss.item()) if loss is not None else float("inf")
# def compute_fluency(text):
#     if len(text.strip()) < 10:
#         return float("inf")  # Too short to evaluate

#     prompt = f"അതിനെ എതിർക്കുന്നു: {text.strip()}"
#     inputs = gen_tokenizer(prompt, return_tensors="pt", truncation=True, padding=True, max_length=128).to(gen_model.device)

#     with torch.no_grad():
#         outputs = gen_model(input_ids=inputs["input_ids"], labels=inputs["input_ids"])
#         loss = outputs.loss

#     if loss is None or loss.item() > 20:  # cap on extreme values
#         return float("inf")

#     return np.exp(loss.item())


def predict_stance(text):
    inputs = stance_tokenizer(text, return_tensors="pt", truncation=True, padding=True).to(stance_model.device)
    with torch.no_grad():
        logits = stance_model(**inputs).logits
    probs = torch.softmax(logits, dim=-1)
    label = torch.argmax(probs, dim=-1).item()
    return "COUNTER" if label == 1 else "HATE", probs[0][1].item()

# ---- Evaluate the generated response ----
generated_cs = response
# reference_cs = "മാനവ അവകാശങ്ങൾ എല്ലാവർക്കും ബാധകമാണ്."  # Optional if you have a gold response
referece_cs = 'അതുകൊണ്ടുതന്നെ, അവർ നിങ്ങൾക്ക് ഒരു പ്രശ്നവും ഉണ്ടാക്കാത്ത കാലത്തോളം, മറ്റുള്ളവർ എങ്ങനെ ജീവിക്കണം എന്ന് നിർദേശിക്കാൻ നിങ്ങൾക്ക് യാതൊരു അവകാശവുമില്ല. ഓരോ വ്യക്തിക്കും അവരുടെ ജീവിതം തങ്ങൾക്ക് ഇഷ്ടമുള്ള രീതിയിൽ നയിക്കാൻ സ്വാതന്ത്ര്യമുണ്ട്. "ജീവിക്കാൻ അനുവദിക്കില്ല" എന്ന് കമൻഡ് അടിക്കാൻ ആരും അവകാശവാനല്ല. ഒരു ജനാധിപത്യ സമൂഹത്തിൽ, എല്ലാ വ്യക്തികളെയും ബഹുമാനിക്കുകയും, അവരുടെ അവകാശങ്ങളെ അംഗീകരിക്കുകയും ചെയ്യേണ്ടതുണ്ട്. എല്ലാവരും സമത്വത്തോടെ ജീവിക്കേണ്ടതാണെന്നത് മനസ്സിലാക്കുക എന്നതാകും ഏറ്റവും ഉചിതം.'

metrics = {
    "Relevance": compute_relevance(user_input, generated_cs),
    "Novelty": compute_novelty(generated_cs, [cs for cs, _ in retrieve_topk_semantic(user_input, top_k=30)]),
    "Fluency_PPL": compute_fluency(safe_cs),
    "Toxicity": compute_toxicity(generated_cs),
    # "BLEU": compute_bleu(reference_cs, generated_cs),
}

stance_label, stance_conf = predict_stance(generated_cs)
metrics["Stance"] = stance_label
metrics["Stance_Confidence"] = stance_conf

# Display metrics
for k, v in metrics.items():
    print(f"{k}: {v:.4f}" if isinstance(v, float) else f"{k}: {v}")

print("Generated Counter-Speech:")
print(generated_cs)

def compute_fluency(text):
    if len(text.strip()) < 10:
        return float("inf")  # Too short to evaluate

    prompt = f"അതിനെ എതിർക്കുന്നു: {text.strip()}"
    inputs = gen_tokenizer(prompt, return_tensors="pt", truncation=True, padding=True, max_length=128).to(gen_model.device)

    with torch.no_grad():
        outputs = gen_model(input_ids=inputs["input_ids"], labels=inputs["input_ids"])
        loss = outputs.loss

    if loss is None or loss.item() > 20:  # cap on extreme values
        return float("inf")

    return np.exp(loss.item())

!pip list --not-required

