# -*- coding: utf-8 -*-
"""ReZG_Train+Gen_Full_Code.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1cgtfToOYgW1w-nlSgB6aQ_IYhdDKhdiR
"""

import pandas as pd
from sklearn.model_selection import train_test_split
from datasets import Dataset
from transformers import XLMRobertaTokenizer, XLMRobertaForSequenceClassification, Trainer, TrainingArguments
from transformers import DataCollatorWithPadding
import numpy as np
import torch
from sklearn.metrics import accuracy_score, precision_recall_fscore_support
import os

# Disable Weights & Biases logging
os.environ["WANDB_DISABLED"] = "true"

# Load the dataset
file_path = "5100GPTDatasetHITL.csv.csv"
df = pd.read_csv(file_path)

# Prepare data
data = pd.DataFrame({
    "text": df["H/T"].astype(str).tolist() + df["CS"].astype(str).tolist(),
    "label": [0] * len(df) + [1] * len(df)  # 0 = HATE, 1 = COUNTER
}).sample(frac=1.0, random_state=42).reset_index(drop=True)

# Train-test split
train_df, test_df = train_test_split(data, test_size=0.2, stratify=data["label"])

# Tokenizer
tokenizer = XLMRobertaTokenizer.from_pretrained("xlm-roberta-base")

def tokenize_function(examples):
    return tokenizer(examples["text"], truncation=True)

# Convert to Hugging Face Datasets
train_ds = Dataset.from_pandas(train_df)
test_ds = Dataset.from_pandas(test_df)
train_ds = train_ds.map(tokenize_function, batched=True)
test_ds = test_ds.map(tokenize_function, batched=True)

# Model
model = XLMRobertaForSequenceClassification.from_pretrained("xlm-roberta-base", num_labels=2)

# Data collator
data_collator = DataCollatorWithPadding(tokenizer=tokenizer)

# Metrics
def compute_metrics(eval_pred):
    logits, labels = eval_pred
    predictions = np.argmax(logits, axis=-1)
    precision, recall, f1, _ = precision_recall_fscore_support(labels, predictions, average="binary")
    acc = accuracy_score(labels, predictions)
    return {"accuracy": acc, "f1": f1, "precision": precision, "recall": recall}

# Training arguments
training_args = TrainingArguments(
    output_dir="./results",
    eval_strategy="epoch",
    save_strategy="epoch",
    learning_rate=2e-5,
    per_device_train_batch_size=8,
    per_device_eval_batch_size=8,
    num_train_epochs=20,
    weight_decay=0.01,
    save_total_limit=1,
    load_best_model_at_end=True,
)

# Trainer
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_ds,
    eval_dataset=test_ds,
    tokenizer=tokenizer,
    data_collator=data_collator,
    compute_metrics=compute_metrics,
)

# Train and save
trainer.train()
model.save_pretrained("saved_model")
tokenizer.save_pretrained("saved_model")



import shutil

# Zip the folder
shutil.make_archive('saved_model', 'zip', 'saved_model')

# Download the zipped folder
from google.colab import files
files.download('saved_model.zip')

!pip install faiss-cpu

# Convert your dataset to JSONL format suitable for fine-tuning

output_path = "fine_tune_data.jsonl"

# Filter and rename
fine_tune_data = df[["H/T", "CS"]].dropna()
fine_tune_data = fine_tune_data.rename(columns={"H/T": "instruction", "CS": "response"})

# Format instructions
fine_tune_data["instruction"] = fine_tune_data["instruction"].apply(
    lambda x: f"‡¥π‡µá‡¥±‡µç‡¥±‡µÅ‡¥ï‡µæ: {x.strip()}"
)

# Export as JSONL
fine_tune_data.to_json(output_path, orient="records", lines=True, force_ascii=False)

# Display path for download
output_path

!pip install transformers datasets peft accelerate bitsandbytes

!pip install -U bitsandbytes
!pip install -U transformers accelerate peft

from datasets import Dataset
from transformers import (
    AutoTokenizer, AutoModelForCausalLM,
    TrainingArguments, Trainer
)
from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training
import pandas as pd
import os
import torch

# Disable WandB
os.environ["WANDB_DISABLED"] = "true"

# ‚úÖ Load the dataset
df = pd.read_json("fine_tune_data.jsonl", lines=True)
dataset = Dataset.from_pandas(df)

# Optional: Remove index column
if "__index_level_0__" in dataset.column_names:
    dataset = dataset.remove_columns("__index_level_0__")

# ‚úÖ Format as instruction prompt
def format_prompt(example):
    return f"‡¥µ‡¥æ‡¥ï‡µç‡¥Ø‡¥§‡µç‡¥§‡¥ø‡µΩ ‡¥§‡µÜ‡¥≥‡¥ø‡¥Ø‡¥ø‡¥ï‡µç‡¥ï‡µÅ‡¥®‡µç‡¥® ‡¥µ‡µÜ‡¥±‡µÅ‡¥™‡µç‡¥™‡¥ø‡¥®‡µç ‡¥é‡¥§‡¥ø‡¥∞‡¥æ‡¥Ø ‡¥í‡¥∞‡µÅ ‡¥Æ‡¥±‡µÅ‡¥™‡¥ü‡¥ø ‡¥é‡¥¥‡µÅ‡¥§‡µÅ‡¥ï.\n\n{example['instruction']}\n\n‡¥Æ‡¥±‡µÅ‡¥™‡¥ü‡¥ø: {example['response']}"

dataset = dataset.map(lambda x: {"text": format_prompt(x)})

# ‚úÖ Load tokenizer and model
model_name = "VinkuraAI/KunoRZN-Llama-3-3B"
tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=False)
model = AutoModelForCausalLM.from_pretrained(model_name, load_in_4bit=True, device_map="auto")

# ‚úÖ Tokenization with labels
def tokenize(example):
    result = tokenizer(
        example["text"],
        truncation=True,
        padding="max_length",
        max_length=512,
    )
    result["labels"] = result["input_ids"].copy()
    return result

tokenized_dataset = dataset.map(tokenize, batched=True)

# ‚úÖ Apply QLoRA configuration
model = prepare_model_for_kbit_training(model)
lora_config = LoraConfig(
    r=8,
    lora_alpha=16,
    target_modules=["q_proj", "v_proj"],
    lora_dropout=0.05,
    bias="none",
    task_type="CAUSAL_LM"
)
model = get_peft_model(model, lora_config)

# ‚úÖ Training arguments
training_args = TrainingArguments(
    output_dir="kuno-finetuned",
    per_device_train_batch_size=2,
    gradient_accumulation_steps=4,
    num_train_epochs=3,
    learning_rate=2e-4,
    fp16=True,
    save_strategy="epoch",
    logging_dir="./logs",
    save_total_limit=2,
)

# ‚úÖ Trainer setup
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_dataset,
    tokenizer=tokenizer
)

# ‚úÖ Train!
trainer.train()

# ‚úÖ Save fine-tuned model
model.save_pretrained("kuno-finetuned")
tokenizer.save_pretrained("kuno-finetuned")

# ---- SSF-Based Counter-Speech Chatbot with KunoRZN-Llama-3-3B Generation ----

# Required installations (uncomment in Colab)
# !pip install transformers sentence-transformers faiss-cpu

import torch
import faiss
import numpy as np
import pandas as pd
import math
from sklearn.metrics.pairwise import cosine_similarity
from transformers import (
    XLMRobertaTokenizer, XLMRobertaForSequenceClassification,
    AutoTokenizer, AutoModelForCausalLM
)
from sentence_transformers import SentenceTransformer



# Load dataset
df = pd.read_csv("5100GPTDatasetHITL.csv.csv")
cs_list = df["CS"].astype(str).tolist()

# === 1. Semantic Similarity ===
embedder = SentenceTransformer("sentence-transformers/LaBSE")
cs_embeddings = embedder.encode(cs_list, convert_to_numpy=True, show_progress_bar=True)
index = faiss.IndexFlatL2(cs_embeddings.shape[1])
index.add(cs_embeddings)

def retrieve_topk_semantic(hs_text, top_k=30):
    hs_embedding = embedder.encode([hs_text], convert_to_numpy=True)
    distances, indices = index.search(hs_embedding, top_k)
    return [(cs_list[i], i) for i in indices[0]]

# === 2. Stance-Based Ranking ===
# Load stance classification model
stance_model_path = "./saved_model"
stance_tokenizer = XLMRobertaTokenizer.from_pretrained(stance_model_path)
stance_model = XLMRobertaForSequenceClassification.from_pretrained(stance_model_path)

# stance_tokenizer = XLMRobertaTokenizer.from_pretrained("xlm-roberta-base")
# stance_model = XLMRobertaForSequenceClassification.from_pretrained("xlm-roberta-base")
stance_model.eval()
stance_model.to("cuda" if torch.cuda.is_available() else "cpu")

def get_cls_embedding(text):
    inputs = stance_tokenizer(text, return_tensors="pt", truncation=True, padding=True, max_length=128)
    inputs = {k: v.to(stance_model.device) for k, v in inputs.items()}
    with torch.no_grad():
        outputs = stance_model.base_model(**inputs)
    return outputs.last_hidden_state[:, 0, :].squeeze().cpu().numpy()

def rank_by_stance(hs_text, retrieved_cs, top_k=10):
    hs_vec = get_cls_embedding(hs_text)
    cs_vecs = [get_cls_embedding(cs) for cs, _ in retrieved_cs]
    sims = cosine_similarity([hs_vec], cs_vecs)[0]
    sorted_indices = np.argsort(sims)
    return [retrieved_cs[i][0] for i in sorted_indices[:top_k]]

# === 3. Fitness (Perplexity) ===
from transformers import AutoModelForSeq2SeqLM
gen_tokenizer = AutoTokenizer.from_pretrained("google/mt5-small")
gen_model = AutoModelForSeq2SeqLM.from_pretrained("google/mt5-small")
gen_model.eval()
gen_model.to("cuda" if torch.cuda.is_available() else "cpu")

def compute_perplexity(prompt):
    encodings = gen_tokenizer(prompt, return_tensors="pt", truncation=True, padding=True, max_length=128)
    input_ids = encodings.input_ids.to(gen_model.device)
    with torch.no_grad():
        outputs = gen_model(input_ids=input_ids, labels=input_ids)
        loss = outputs.loss
    return math.exp(loss.item()) if loss is not None else float("inf")

def rank_by_fitness(hs_text, cs_candidates, top_k=10):
    prompt_template = lambda cs: f"{hs_text} ‡¥Ö‡¥§‡¥ø‡¥®‡µÜ ‡¥é‡¥§‡¥ø‡µº‡¥ï‡µç‡¥ï‡µÅ‡¥®‡µç‡¥®‡µÅ: {cs}"
    ranked = [(cs, compute_perplexity(prompt_template(cs))) for cs in cs_candidates]
    return sorted(ranked, key=lambda x: x[1])[:top_k]

# === 4. Final Generation using VinkuraAI/KunoRZN-Llama-3-3B ===
# kuno_tokenizer = AutoTokenizer.from_pretrained("VinkuraAI/KunoRZN-Llama-3-3B")
# kuno_model = AutoModelForCausalLM.from_pretrained("VinkuraAI/KunoRZN-Llama-3-3B")
kuno_model = AutoModelForCausalLM.from_pretrained("kuno-finetuned")
kuno_tokenizer = AutoTokenizer.from_pretrained("kuno-finetuned")

kuno_model.eval()
kuno_model.to("cuda" if torch.cuda.is_available() else "cpu")

def generate_final_response(hs_text, top_cs):
    prompt = f"""‡¥¶‡¥Ø‡¥µ‡¥æ‡¥Ø‡¥ø ‡¥í‡¥∞‡µÅ ‡¥∂‡¥æ‡¥®‡µç‡¥§‡¥Æ‡¥æ‡¥Ø, ‡¥¨‡¥π‡µÅ‡¥Æ‡¥æ‡¥®‡¥™‡¥∞‡¥Æ‡¥æ‡¥Ø, ‡¥µ‡¥ø‡¥∂‡¥¶‡µÄ‡¥ï‡¥∞‡¥£‡¥™‡¥∞‡¥Æ‡¥æ‡¥Ø ‡¥Æ‡¥±‡µÅ‡¥™‡¥ü‡¥ø ‡¥§‡¥Ø‡µç‡¥Ø‡¥æ‡¥±‡¥æ‡¥ï‡µç‡¥ï‡µÅ‡¥ï.
‡¥§‡¥æ‡¥¥‡µÜ‡¥Ø‡µÅ‡¥≥‡µç‡¥≥ ‡¥µ‡µÜ‡¥±‡µÅ‡¥™‡µç‡¥™‡µç ‡¥™‡µç‡¥∞‡¥ï‡¥ü‡¥ø‡¥™‡µç‡¥™‡¥ø‡¥ï‡µç‡¥ï‡µÅ‡¥®‡µç‡¥® ‡¥µ‡¥æ‡¥ï‡µç‡¥Ø‡¥§‡µç‡¥§‡¥ø‡¥®‡µç‡¥í, ‡¥í‡¥∞‡µÅ ‡¥Ö‡¥±‡¥ø‡¥µ‡µç ‡¥Ö‡¥ü‡¥ø‡¥∏‡µç‡¥•‡¥æ‡¥®‡¥Æ‡¥æ‡¥ï‡µç‡¥ï‡¥ø‡¥Ø‡µÅ‡¥≥‡µç‡¥≥ ‡¥é‡¥§‡¥ø‡¥∞‡µç‚Äç‡¥™‡µç ‡¥∞‡µá‡¥ñ‡¥™‡µç‡¥™‡µÜ‡¥ü‡µÅ‡¥§‡µç‡¥§‡µÅ‡¥ï.
‡¥Æ‡¥±‡µÅ‡¥™‡¥ü‡¥ø ‡¥§‡µº‡¥ï‡µç‡¥ï‡¥Ç ‡¥ï‡µç‡¥∑‡¥£‡¥ø‡¥ï‡µç‡¥ï‡¥æ‡¥§‡µÜ, ‡¥Ø‡µÅ‡¥ï‡µç‡¥§‡¥ø‡¥™‡µÇ‡µº‡¥£‡µç‡¥£‡¥µ‡µÅ‡¥Ç ‡¥Æ‡¥æ‡¥®‡µç‡¥Ø‡¥µ‡µÅ‡¥Æ‡¥æ‡¥Ø‡¥ø‡¥∞‡¥ø‡¥ï‡µç‡¥ï‡µÅ‡¥ï.

‡¥π‡µá‡¥±‡µç‡¥±‡µÅ‡¥ï‡µæ: {hs_text}
‡¥â‡¥¶‡¥æ‡¥π‡¥∞‡¥£ ‡¥™‡µç‡¥∞‡¥§‡¥ø‡¥ï‡¥∞‡¥£‡¥Ç: {top_cs}

‡¥Æ‡¥±‡µÅ‡¥™‡¥ü‡¥ø:"""
    inputs = kuno_tokenizer(prompt, return_tensors="pt").to(kuno_model.device)
    with torch.no_grad():
        output_ids = kuno_model.generate(
            **inputs,
            max_new_tokens=255,
            do_sample=True,
            top_k=50,
            top_p=0.95,
            temperature=0.7
        )
    return kuno_tokenizer.decode(output_ids[0], skip_special_tokens=True).split("‡¥Æ‡¥±‡µÅ‡¥™‡¥ü‡¥ø:")[-1].strip()

# === Full Chatbot Function ===
def generate_counter_speech(hs_input):
    sem_candidates = retrieve_topk_semantic(hs_input, top_k=30)
    stance_filtered = rank_by_stance(hs_input, sem_candidates, top_k=10)
    top_fluent = rank_by_fitness(hs_input, stance_filtered, top_k=1)
    print(f"Top Fluent: {top_fluent}")
    if not top_fluent:
        return "‡¥ï‡µç‡¥∑‡¥Æ‡¥ø‡¥ï‡µç‡¥ï‡¥£‡¥Ç, ‡¥Ö‡¥§‡¥ø‡¥®‡¥æ‡¥Ø‡µÅ‡¥≥‡µç‡¥≥ ‡¥Æ‡¥±‡µÅ‡¥™‡¥ü‡¥ø ‡¥ï‡¥£‡µç‡¥ü‡µÜ‡¥§‡µç‡¥§‡¥æ‡µª ‡¥ï‡¥¥‡¥ø‡¥û‡µç‡¥û‡¥ø‡¥≤‡µç‡¥≤."
    return generate_final_response(hs_input, top_fluent[0][0])

# # === Example Use ===
# user_input = "‡¥Ö‡¥µ‡µº‡¥ï‡µç‡¥ï‡µç ‡¥µ‡¥ø‡¥µ‡¥æ‡¥π‡¥Ç ‡¥ï‡¥¥‡¥ø‡¥ï‡µç‡¥ï‡¥æ‡¥®‡µÅ‡¥≥‡µç‡¥≥ ‡¥Ö‡¥µ‡¥ï‡¥æ‡¥∂‡¥Æ‡¥ø‡¥≤‡µç‡¥≤"
# response = generate_counter_speech(user_input)
# print(f"ü§ñ: {response}")

user_input = "‡¥Ö‡¥µ‡µº‡¥ï‡µç‡¥ï‡µç ‡¥µ‡¥ø‡¥µ‡¥æ‡¥π‡¥Ç ‡¥ï‡¥¥‡¥ø‡¥ï‡µç‡¥ï‡¥æ‡¥®‡µÅ‡¥≥‡µç‡¥≥ ‡¥Ö‡¥µ‡¥ï‡¥æ‡¥∂‡¥Æ‡¥ø‡¥≤‡µç‡¥≤"
response = generate_counter_speech(user_input)
print(f"ü§ñ: {response}")

print(f"ü§ñ: {response[1]}")

!pip install detoxify

from detoxify import Detoxify
from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction
import nltk
nltk.download('punkt_tab')


# Load Detoxify
tox_model = Detoxify("original")
# generated_cs = "‡¥®‡¥ø‡¥®‡¥ø‡¥ï‡µç‡¥ï‡µç ‡¥Ö‡¥§‡µÅ‡¥ï‡µä‡¥£‡µç‡¥ü‡µÅ‡¥§‡¥®‡µç‡¥®‡µÜ, ‡¥Ö‡¥µ‡µº ‡¥®‡¥ø‡¥ô‡µç‡¥ô‡µæ‡¥ï‡µç‡¥ï‡µç ‡¥Ö‡¥§‡µç‡¥∞‡¥Ø‡¥ø‡¥≤‡µá‡¥±‡µç‡¥±‡¥ø‡µΩ ‡¥Ü‡¥ó‡µç‡¥∞‡¥π‡¥ø‡¥ï‡µç‡¥ï‡µÅ‡¥®‡µç‡¥® ‡¥µ‡¥ø‡¥ß‡¥§‡µç‡¥§‡¥ø‡µΩ ‡¥ú‡µÄ‡¥µ‡¥ø‡¥ï‡µç‡¥ï‡¥æ‡¥®‡µÅ‡¥≥‡µç‡¥≥ ‡¥Ö‡¥µ‡¥ï‡¥æ‡¥∂‡¥µ‡µÅ‡¥Ç ‡¥â‡¥≥‡µç‡¥≥‡¥§‡¥æ‡¥Ø‡¥ø‡¥∞‡¥ø‡¥ï‡µç‡¥ï‡µÅ‡¥ï‡¥Ø‡¥ø‡¥≤‡µç‡¥≤. ‡¥í‡¥∞‡µÅ ‡¥µ‡¥ø‡¥µ‡¥æ‡¥π‡¥§‡µç‡¥§‡¥ø‡µΩ ‡¥™‡¥ô‡µç‡¥ï‡µÜ‡¥ü‡µÅ‡¥ï‡µç‡¥ï‡µÅ‡¥®‡µç‡¥®‡¥§‡µç ‡¥Ö‡¥µ"

def clean_generated_text(text):
    if "." in text:
        return text.rsplit(".", 1)[0] + "."
    return text  # fallback

safe_cs = clean_generated_text(response)

# ---- Evaluation Functions ----
def compute_relevance(hs_text, cs_text):
    hs_vec = embedder.encode([hs_text])
    cs_vec = embedder.encode([cs_text])
    return cosine_similarity(hs_vec, cs_vec)[0][0]

def compute_novelty(cs_text, training_cs_list):
    cs_vec = embedder.encode([cs_text])
    train_vecs = embedder.encode(training_cs_list)
    sims = cosine_similarity(cs_vec, train_vecs)[0]
    return 1 - np.max(sims)

def compute_bleu(reference, candidate):
    ref_tokens = [nltk.word_tokenize(reference)]
    cand_tokens = nltk.word_tokenize(candidate)
    return sentence_bleu(ref_tokens, cand_tokens, smoothing_function=SmoothingFunction().method1)

def compute_toxicity(text):
    return tox_model.predict(text).get("toxicity", 0.0)

def compute_fluency(text):
    inputs = gen_tokenizer(text, return_tensors="pt", truncation=True, padding=True, max_length=128).to(gen_model.device)
    with torch.no_grad():
        outputs = gen_model(input_ids=inputs["input_ids"], labels=inputs["input_ids"])
        loss = outputs.loss
    return np.exp(loss.item()) if loss is not None else float("inf")
# def compute_fluency(text):
#     if len(text.strip()) < 10:
#         return float("inf")  # Too short to evaluate

#     prompt = f"‡¥Ö‡¥§‡¥ø‡¥®‡µÜ ‡¥é‡¥§‡¥ø‡µº‡¥ï‡µç‡¥ï‡µÅ‡¥®‡µç‡¥®‡µÅ: {text.strip()}"
#     inputs = gen_tokenizer(prompt, return_tensors="pt", truncation=True, padding=True, max_length=128).to(gen_model.device)

#     with torch.no_grad():
#         outputs = gen_model(input_ids=inputs["input_ids"], labels=inputs["input_ids"])
#         loss = outputs.loss

#     if loss is None or loss.item() > 20:  # cap on extreme values
#         return float("inf")

#     return np.exp(loss.item())


def predict_stance(text):
    inputs = stance_tokenizer(text, return_tensors="pt", truncation=True, padding=True).to(stance_model.device)
    with torch.no_grad():
        logits = stance_model(**inputs).logits
    probs = torch.softmax(logits, dim=-1)
    label = torch.argmax(probs, dim=-1).item()
    return "COUNTER" if label == 1 else "HATE", probs[0][1].item()

# ---- Evaluate the generated response ----
generated_cs = response
# reference_cs = "‡¥Æ‡¥æ‡¥®‡¥µ ‡¥Ö‡¥µ‡¥ï‡¥æ‡¥∂‡¥ô‡µç‡¥ô‡µæ ‡¥é‡¥≤‡µç‡¥≤‡¥æ‡¥µ‡µº‡¥ï‡µç‡¥ï‡µÅ‡¥Ç ‡¥¨‡¥æ‡¥ß‡¥ï‡¥Æ‡¥æ‡¥£‡µç."  # Optional if you have a gold response
referece_cs = '‡¥Ö‡¥§‡µÅ‡¥ï‡µä‡¥£‡µç‡¥ü‡µÅ‡¥§‡¥®‡µç‡¥®‡µÜ, ‡¥Ö‡¥µ‡µº ‡¥®‡¥ø‡¥ô‡µç‡¥ô‡µæ‡¥ï‡µç‡¥ï‡µç ‡¥í‡¥∞‡µÅ ‡¥™‡µç‡¥∞‡¥∂‡µç‡¥®‡¥µ‡µÅ‡¥Ç ‡¥â‡¥£‡µç‡¥ü‡¥æ‡¥ï‡µç‡¥ï‡¥æ‡¥§‡µç‡¥§ ‡¥ï‡¥æ‡¥≤‡¥§‡µç‡¥§‡µã‡¥≥‡¥Ç, ‡¥Æ‡¥±‡µç‡¥±‡µÅ‡¥≥‡µç‡¥≥‡¥µ‡µº ‡¥é‡¥ô‡µç‡¥ô‡¥®‡µÜ ‡¥ú‡µÄ‡¥µ‡¥ø‡¥ï‡µç‡¥ï‡¥£‡¥Ç ‡¥é‡¥®‡µç‡¥®‡µç ‡¥®‡¥ø‡µº‡¥¶‡µá‡¥∂‡¥ø‡¥ï‡µç‡¥ï‡¥æ‡µª ‡¥®‡¥ø‡¥ô‡µç‡¥ô‡µæ‡¥ï‡µç‡¥ï‡µç ‡¥Ø‡¥æ‡¥§‡µä‡¥∞‡µÅ ‡¥Ö‡¥µ‡¥ï‡¥æ‡¥∂‡¥µ‡µÅ‡¥Æ‡¥ø‡¥≤‡µç‡¥≤. ‡¥ì‡¥∞‡µã ‡¥µ‡µç‡¥Ø‡¥ï‡µç‡¥§‡¥ø‡¥ï‡µç‡¥ï‡µÅ‡¥Ç ‡¥Ö‡¥µ‡¥∞‡µÅ‡¥ü‡µÜ ‡¥ú‡µÄ‡¥µ‡¥ø‡¥§‡¥Ç ‡¥§‡¥ô‡µç‡¥ô‡µæ‡¥ï‡µç‡¥ï‡µç ‡¥á‡¥∑‡µç‡¥ü‡¥Æ‡µÅ‡¥≥‡µç‡¥≥ ‡¥∞‡µÄ‡¥§‡¥ø‡¥Ø‡¥ø‡µΩ ‡¥®‡¥Ø‡¥ø‡¥ï‡µç‡¥ï‡¥æ‡µª ‡¥∏‡µç‡¥µ‡¥æ‡¥§‡¥®‡µç‡¥§‡µç‡¥∞‡µç‡¥Ø‡¥Æ‡µÅ‡¥£‡µç‡¥ü‡µç. "‡¥ú‡µÄ‡¥µ‡¥ø‡¥ï‡µç‡¥ï‡¥æ‡µª ‡¥Ö‡¥®‡µÅ‡¥µ‡¥¶‡¥ø‡¥ï‡µç‡¥ï‡¥ø‡¥≤‡µç‡¥≤" ‡¥é‡¥®‡µç‡¥®‡µç ‡¥ï‡¥Æ‡µª‡¥°‡µç ‡¥Ö‡¥ü‡¥ø‡¥ï‡µç‡¥ï‡¥æ‡µª ‡¥Ü‡¥∞‡µÅ‡¥Ç ‡¥Ö‡¥µ‡¥ï‡¥æ‡¥∂‡¥µ‡¥æ‡¥®‡¥≤‡µç‡¥≤. ‡¥í‡¥∞‡µÅ ‡¥ú‡¥®‡¥æ‡¥ß‡¥ø‡¥™‡¥§‡µç‡¥Ø ‡¥∏‡¥Æ‡µÇ‡¥π‡¥§‡µç‡¥§‡¥ø‡µΩ, ‡¥é‡¥≤‡µç‡¥≤‡¥æ ‡¥µ‡µç‡¥Ø‡¥ï‡µç‡¥§‡¥ø‡¥ï‡¥≥‡µÜ‡¥Ø‡µÅ‡¥Ç ‡¥¨‡¥π‡µÅ‡¥Æ‡¥æ‡¥®‡¥ø‡¥ï‡µç‡¥ï‡µÅ‡¥ï‡¥Ø‡µÅ‡¥Ç, ‡¥Ö‡¥µ‡¥∞‡µÅ‡¥ü‡µÜ ‡¥Ö‡¥µ‡¥ï‡¥æ‡¥∂‡¥ô‡µç‡¥ô‡¥≥‡µÜ ‡¥Ö‡¥Ç‡¥ó‡µÄ‡¥ï‡¥∞‡¥ø‡¥ï‡µç‡¥ï‡µÅ‡¥ï‡¥Ø‡µÅ‡¥Ç ‡¥ö‡µÜ‡¥Ø‡µç‡¥Ø‡µá‡¥£‡µç‡¥ü‡¥§‡µÅ‡¥£‡µç‡¥ü‡µç. ‡¥é‡¥≤‡µç‡¥≤‡¥æ‡¥µ‡¥∞‡µÅ‡¥Ç ‡¥∏‡¥Æ‡¥§‡µç‡¥µ‡¥§‡µç‡¥§‡µã‡¥ü‡µÜ ‡¥ú‡µÄ‡¥µ‡¥ø‡¥ï‡µç‡¥ï‡µá‡¥£‡µç‡¥ü‡¥§‡¥æ‡¥£‡µÜ‡¥®‡µç‡¥®‡¥§‡µç ‡¥Æ‡¥®‡¥∏‡µç‡¥∏‡¥ø‡¥≤‡¥æ‡¥ï‡µç‡¥ï‡µÅ‡¥ï ‡¥é‡¥®‡µç‡¥®‡¥§‡¥æ‡¥ï‡µÅ‡¥Ç ‡¥è‡¥±‡µç‡¥±‡¥µ‡µÅ‡¥Ç ‡¥â‡¥ö‡¥ø‡¥§‡¥Ç.'

metrics = {
    "Relevance": compute_relevance(user_input, generated_cs),
    "Novelty": compute_novelty(generated_cs, [cs for cs, _ in retrieve_topk_semantic(user_input, top_k=30)]),
    "Fluency_PPL": compute_fluency(safe_cs),
    "Toxicity": compute_toxicity(generated_cs),
    # "BLEU": compute_bleu(reference_cs, generated_cs),
}

stance_label, stance_conf = predict_stance(generated_cs)
metrics["Stance"] = stance_label
metrics["Stance_Confidence"] = stance_conf

# Display metrics
for k, v in metrics.items():
    print(f"{k}: {v:.4f}" if isinstance(v, float) else f"{k}: {v}")

print("Generated Counter-Speech:")
print(generated_cs)

def compute_fluency(text):
    if len(text.strip()) < 10:
        return float("inf")  # Too short to evaluate

    prompt = f"‡¥Ö‡¥§‡¥ø‡¥®‡µÜ ‡¥é‡¥§‡¥ø‡µº‡¥ï‡µç‡¥ï‡µÅ‡¥®‡µç‡¥®‡µÅ: {text.strip()}"
    inputs = gen_tokenizer(prompt, return_tensors="pt", truncation=True, padding=True, max_length=128).to(gen_model.device)

    with torch.no_grad():
        outputs = gen_model(input_ids=inputs["input_ids"], labels=inputs["input_ids"])
        loss = outputs.loss

    if loss is None or loss.item() > 20:  # cap on extreme values
        return float("inf")

    return np.exp(loss.item())

!pip list --not-required

